{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LinearRegression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPOLQ8B1cU0P0vWgR6EDE2o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ReshamWadhwa/ML-models/blob/main/LinearRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZQXSe8MeP5S"
      },
      "source": [
        "# Linear Regression"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmRtDI9zefim"
      },
      "source": [
        "# Feature Vector \n",
        "# x = [x_1, x_2, …., x_n]\n",
        "# REsponse vector\n",
        "# y = [y_1, y_2, …., y_n]\n",
        "\n",
        "#  h( x ) = w * x + b  \n",
        "    \n",
        "#   b = bias -- needed\n",
        "#   x represents the feature vector\n",
        "#   w represents the weight vector.\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02tSlBl4njxF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYun4JI1nm3A"
      },
      "source": [
        "**WHY IS BIAS TERM NEEDED?**\n",
        "\n",
        "When bias is absent\n",
        "\n",
        "y = w1x1+w2x2+w3x3...+wnxn\n",
        "\n",
        "For situation when x1=x2=x3 = 0 , our y is forced to be 0.\n",
        "This might lead to underfit or bias in model if original data fit doesn;t pass through 0 but we force it to do so. \n",
        "Hence to avoid bias wrt origin, a bias term is added to the equation and the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbUnrxNIoEhI"
      },
      "source": [
        "What are the assumptions about data before fitting a Linear Regressor?\n",
        "\n",
        "1. **Data Target variable is continuous (Regression problem).**\n",
        "    \n",
        "    Non continous data require classification model\n",
        "\n",
        "1. **Linear relationship in y and X.**\n",
        "\n",
        "  Relationship between dependent and independent variables is linear. \n",
        "  \n",
        "  The linearity assumption can be tested using scatter plots.\n",
        "\n",
        "  **But Why?** Because Non linear data relationships cannot be captured by sum of scalar matrix multiplications - how LR works\n",
        "\n",
        "1. **Little or no multi-collinearity.**\n",
        "\n",
        "  Features shouldn't be dependent on each other. \n",
        "\n",
        "  There shouldn't be high correlation between two or more independent variables i.e., X. \n",
        "\n",
        "  **But why?** Because coefficients/weights are reflective of change in that feature only while computing Y. \n",
        "  Let say , y = w1a1+w2a2+w3a3+ w0   //(bias)\n",
        "\n",
        "  and  that a1 and a2 are correlated.\n",
        "\n",
        "  Now, model's interpretation of w1 is the change in y caused by a unit change in a1 alone and no other feature, however since a1 and a2 are correlated, this assumption fails.\n",
        "\n",
        "  Can be resolved using VIF ( Variance Inflation Factor )\n",
        "\n",
        "1. **Little or no auto-correlation/Independence of observations.**\n",
        "  \n",
        "    Autocorrelation is when a variable is related to earlier versions of itself. \n",
        "\n",
        "1. **Homoscedasticity**\n",
        "\n",
        "  Homoscedasticity describes a situation in which the error term (that is, the “noise” or random disturbance in the relationship between the independent variables and the dependent variable) is the same across all values of the independent variables. The variance of the residuals is constant.\n",
        "\n",
        "  **But why ?** 10% change in lower value of x will result in lower error than 10% change in x when it is equal to 1,000,000. This is a case when error will have different variance across different range of features.\n",
        "\n",
        "  Can be solved by using Weighted Least Square Model - This type of regression assigns a weight to each data point based on the variance of its fitted value.\n",
        "\n",
        "1. **All independent variables are uncorrelated with the error term**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jy8fzv6TsQwi"
      },
      "source": [
        "**Variance inflation factor (VIF)** \n",
        "\n",
        "is used to detect the severity of multicollinearity in the ordinary least square (OLS) regression analysis.\n",
        "\n",
        "\n",
        "Variance Inflation Factors (VIFs) measure the correlation among independent variables in least squares regression models. Statisticians refer to this type of correlation as multicollinearity. Excessive multicollinearity can cause problems for regression models.\n",
        "\n",
        "*Calculating Variance Inflation Factors*\n",
        "\n",
        "VIFs use multiple regression to calculate the degree of multicollinearity. Imagine you have four independent variables: X1, X2, X3, and X4. Of course, the model has a dependent variable (Y), but we don’t need to worry about it for our purposes. When your statistical software calculates VIFs, it uses multiple regression to regress all IVs except one on that final IV. It repeats this process for all IVs, as shown below:\n",
        "\n",
        "X1 ⇐ X2, X3, X4\n",
        "X2 ⇐ X1, X3, X4\n",
        "X3 ⇐ X1, X2, X4\n",
        "X4 ⇐ X1, X2, X3\n",
        "\n",
        "To calculate the VIFs, all independent variables become a dependent variable. Each model produces an R-squared value indicating the percentage of the variance in the individual IV that the set of IVs explains. Consequently, higher R-squared values indicate higher degrees of multicollinearity. VIF calculations use these R-squared values. The VIF for an independent variable equals the following:\n",
        "\n",
        "VIF formula.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGoXwxmlvx8h"
      },
      "source": [
        "\n",
        "**R-squared** = Variance Explained (R2)\n",
        "\n",
        "It records the proportion of variation in the dependent variable explained by the independent variables. \n",
        "In range [0,1] -- 0 means X doesn't explain any variance in Y and hence has no impact at all, 1 means X explains all variation present in Y\n",
        "\n",
        "In case of overfit, R2 will still be high if too many independent features are present . Adjusted R2 considers the nummber of features as well. \n",
        "\n",
        "\n",
        "```\n",
        "Adjusted R Squared = 1 – [((1 – R2) * (n – 1)) / (n – k – 1)]\n",
        "\n",
        "n == data points \n",
        "k == number of independent feature variables\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "Now, in VIF, all independent features are modelled against other IFs. R-squared is coefficient of determination of these individual features. \n",
        "\n",
        "If R-sq of X1 =0 , it means that other features do not explain any of its variance. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUVqjnUAv0yG"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "Tolerance of i-th feature= 1- R-squared\n",
        "```\n",
        "\n",
        "i.e., how much variance is left unexplained when using the given features.\n",
        "\n",
        "\n",
        "```\n",
        "VIF of i-th feature = 1/tolerance\n",
        "\n",
        "VIFi = 1/(1-R2)\n",
        "```\n",
        "\n",
        "Higher the explained variance for a coef, lower the tolerance, higher the VIF."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGkzFCvzvz8r"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sSxM6BTtKJw"
      },
      "source": [
        "The potential solutions include the following:\n",
        "\n",
        "1. Remove some of the highly correlated independent variables.\n",
        "1. Linearly combine the independent variables, such as adding them together.\n",
        "1. Perform an analysis designed for highly correlated variables, such as principal components analysis or partial least squares regression.\n",
        "1. LASSO and Ridge regression are advanced forms of regression analysis that can handle multicollinearity. If you know how to perform linear least squares regression, you’ll be able to handle these analyses with just a little additional study."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIGl_PIYmA_W"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oA9wLdiOl9Ft"
      },
      "source": [
        "x = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
        "y = np.array([1, 3, 2, 5, 7, 8, 8, 9, 10, 12])\n",
        "\n",
        "# y = w0 + w1x\n",
        "\n",
        "# Aim is to arrive at w0 , w1 values at which error is minimum.\n",
        "\n",
        "# Error = \n",
        "\n",
        "In Ordinary Least Squares Regression, \n",
        "\n",
        "# J(w0,w1)= 1/2m x Sum(y-y`)^2"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5c8W-Uo_l9_9"
      },
      "source": [
        "# # PseudoCode\n",
        "# 1. Start with random values of w0 and w2\n",
        "# 2. Calculate error term after getting output\n",
        "# 3. repeat till convergence:\n",
        "#     a. Find the change to be made in w0 so that error is min\n",
        "#     b. Find the change to be made in w1 so that error is min\n",
        "#     c. Update the weights \n",
        "#     d. Find predictions\n",
        "\n",
        "# repeat until convergence  {\n",
        "#        tmpi = wi - alpha * dwi          \n",
        "#        wi = tmpi              \n",
        "# }\n",
        "# where alpha is the learning rate."
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAYzNkC_4NDZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}