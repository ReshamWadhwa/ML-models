{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LogisticRegression.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOwhLnlFpGwPJEaLswQDSx0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ReshamWadhwa/ML-models/blob/main/LogisticRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yL8XkB98PC0v"
      },
      "source": [
        "# Logictic Regression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZDJXkd1PM0D"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2M1lyJvutH9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZHSsM4VvneD"
      },
      "source": [
        "**Ques. What are the assumptions of a Logistic Regressor?**\n",
        "\n",
        "Ans. Logistic Regression's assumptions about data are listed below along with their reasons.\n",
        "\n",
        "*1. The Response Variable is Binary*\n",
        "```\n",
        "  If the target/dependendent/response variable is non binary or continuous, then Logistic Regression won't give meaningful results as the sigmoid function + loss function test the predictions wrt dichotomy of classes\n",
        "```\n",
        "*2. Observations are independent of each other*\n",
        "```\n",
        "The observations should not come from repeated measurements of the same individual or be related to each other in any way.The dataset should not contain duplicate or repeated values\n",
        "```\n",
        "*3. There is No Multicollinearity Among Explanatory Variables*\n",
        "```\n",
        "Features shouldn't be dependent on each other.\n",
        "\n",
        "There shouldn't be high correlation between two or more independent variables i.e., X.\n",
        "```\n",
        "But why? \n",
        "```\n",
        "Because coefficients/weights are reflective of change in that feature only while computing Y. Let say , y = w1a1+w2a2+w3a3+ w0 //(bias)\n",
        "\n",
        "and that a1 and a2 are correlated.\n",
        "\n",
        "Now, model's interpretation of w1 is the change in y caused by a unit change in a1 alone and no other feature, however since a1 and a2 are correlated, this assumption fails.\n",
        "```\n",
        "\n",
        "4. There are No Extreme Outliers in the data.\n",
        "\n",
        "1. Large sample size\n",
        "\n",
        "1. Linear relationship between observations and their logits\n",
        "```\n",
        "    Logit(p)  = log(p / (1-p))\n",
        "    \n",
        "    where p is the probability of success\n",
        "    \n",
        "    also known as a log-odds function\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1GHmMnQ7gCj"
      },
      "source": [
        "**Question. What is the equation for Logistic Regression?**\n",
        "\n",
        "**Ans.** \n",
        "\n",
        " Feature Vector : x = [x_1, x_2, â€¦., x_n]\n",
        "\n",
        " Response vector : y = [y_1, y_2, â€¦., y_n] where all y = 0 or 1\n",
        " \n",
        "    h( x ) = sigmoid(w * x + b  )\n",
        "    \n",
        "   b = bias -- needed\n",
        "\n",
        "   x represents the feature vector\n",
        "   \n",
        "   w represents the weight vector.\n",
        "\n",
        "\n",
        "```\n",
        "sigmoid(z) = 1/(1+e^-z)\n",
        "```\n",
        "\n",
        "\n",
        "There are two ways to optimize weights in Logistic Regression :\n",
        "\n",
        "1. Least Squares Optimization (iteratively reweighted least squares).\n",
        "1. Maximum Likelihood Estimation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndj4XTLYvDnJ"
      },
      "source": [
        "**Ques. Why is the bias term added ? bias term = b**\n",
        "\n",
        "**Ans**. For situation when x1=x2=x3 = 0 , our y is forced to be 0.5 (sigmoid of 0 = 0.5) . This might lead to underfit or bias in model if original data fit doesn;t pass through 0 but we force it to do so. Hence to avoid bias wrt the stagnant point 0.5, a bias term is added to the equation and the model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkcUUb3fviex"
      },
      "source": [
        "**Quess. What is the Least Squares Optimization process to update the weights in Logistic Regression?**\n",
        "\n",
        "**Ans.**  The linear part of the model (the weighted sum of the inputs == wx+b ) calculates the **log-odds** of a successful event, specifically, the log-odds that a sample belongs to class 1.\n",
        "\n",
        "```\n",
        "log-odds = w0+w1x1 + w2x2+....\n",
        "```\n",
        "Odds = Probability of success/probability of failure\n",
        "\n",
        "In Binomial distribution (Two possible outcomes: true or false, success or failure, yes or no), probability of failure = 1- probability of success for one event.\n",
        "\n",
        "hence, \n",
        "\n",
        "```\n",
        "odds = p/(1-p)\n",
        "```\n",
        "\n",
        "Log odds is log of this value == Logit == Logistic Unit\n",
        "\n",
        "```\n",
        "log-odd = log(p/(1-p))\n",
        "```\n",
        "\n",
        "Now, this is the linear part of the Logistic Regression classification . \n",
        "\n",
        "Hence, \n",
        "```\n",
        "log ( p/(1-p) ) = w0+w1x1 + w2x2+....+ wnxn\n",
        "```\n",
        "\n",
        "What we eventually want is p and that can be obtained by taking exponents of log odds. \n",
        "\n",
        "```\n",
        "odds = p/(1-p) = exp ( w0 + w1x1 + w2x2... + wnxn)\n",
        "```\n",
        "\n",
        "Now, odds can be converted into probability as follows \n",
        "```\n",
        "odds = p/(1-p)\n",
        "\n",
        "odds(1-p) = p\n",
        "odds - p*odds = p \n",
        "odds = p+p*odds\n",
        "odds = p(1+odds)\n",
        "Hence, \n",
        "p = odds/(1+odds)\n",
        "\n",
        "\n",
        "p = exp( log_odds ) / ( 1 + exp(log_odds))\n",
        "\n",
        "p = (1/exp(-log_odds)) / (1+ 1/exp(-log_odds)\n",
        "\n",
        "Let log_odds = l\n",
        "\n",
        "p = (1/e(-l) ) / (1+ 1/e(-l) )\n",
        "```\n",
        "\n",
        "Taking 1/e(-l) commom in both numerator and denominator and cancelling, \n",
        "```\n",
        "p = 1/ ( 1+e(-log_odds))\n",
        "\n",
        "``` \n",
        "\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_2vtrHwzdsi"
      },
      "source": [
        "**Quess. What is the Maximum Likelihood Estimation wrt Logistic Regression?**\n",
        "\n",
        "**Ans.** MLE is a probabilistic framework for estimating the parameters of a model. We wish to maximize the conditional probability of observing the data (X) given a specific probability distribution and its parameters (W), stated formally as:\n",
        "\n",
        "P(X ; W)\n",
        "\n",
        "i.e., what is the probability of finding X ( combination of all input features values) with W weights. MLE works to maximize this. \n",
        "\n",
        "the joint probability distribution of all observations from the problem domain from 1 to n.\n",
        "```\n",
        "P ( x1, x2 , x3... | W). == likelihood of observing this data point given a set of weights == conditional probability = L ( X | W)\n",
        "```\n",
        "```\n",
        "Joint probability = P ( x1,x2,x3...) = P(x1).P(x2)...\n",
        "```\n",
        "But multiplying small values can be unstable (Probabilities are <1)\n",
        "\n",
        "Hence log is applied. Log fits because it is monotonous like the likelihood functions hence peaks for both shall be same. Also , log of multiplying factors converts to sum which is much easier to interpret. \n",
        "\n",
        "``` \n",
        "ð‘™ð‘œð‘”(ð‘¥^ð‘ . ð‘¥^ð‘ž)=ð‘ ð‘™ð‘œð‘”(ð‘¥)+ð‘ž ð‘™ð‘œð‘”(ð‘¥)\n",
        "```\n",
        "Hence log likelihood is calculated. \n",
        "\n",
        "```\n",
        "Log Likelihood = sum i to n log(P(xi ; W))\n",
        "```\n",
        "\n",
        "However , this is still in terms of maximizing the likelihood. Most training models prefer to minimise a cost function. Hence negative of log likelihood is used.\n",
        "```\n",
        "minimize -sum i to n log(P(xi ; W))\n",
        "```\n",
        "\n",
        "The process of finding parameters such that log likelihood is maximum is called Maximum Likelihood Estimation or MLE.\n",
        "\n",
        "In terms of Machine Learning Models, we used this concept to find the hypotheses h and set of model parameters ( w0,w1..., b) such that it best explains the input features X.\n",
        "\n",
        "\n",
        "Hence, we find the modeling hypothesis that maximizes the likelihood function \n",
        "```\n",
        "maximize sum i to n log(P(xi ; W))\n",
        "```\n",
        "Supervised learning like Logistic Regression learning can be considered as \n",
        "predicting output given the input i.e., P ( y | x) \n",
        "\n",
        "Hence, in logistic regression we aim to \n",
        "```\n",
        "maximize sum i to n log(P(yi|xi ; h))\n",
        "```\n",
        "Now, hypothesis h is our Logistic Model. hence\n",
        "\n",
        "In order to use maximum likelihood, we need to assume a probability distribution. In the case of logistic regression, a Binomial probability distribution is assumed for the data sample, where each example is one outcome of a Bernoulli trial. The Bernoulli distribution has a single parameter: the probability of a successful outcome (p).\n",
        "```\n",
        "P(y=1) = p\n",
        "P(y=0) = 1 â€“ p\n",
        "```\n",
        "\n",
        "The expected value (mean) of the Bernoulli distribution can be calculated as follows:\n",
        "```\n",
        "mean = P(y=1) * 1 + P(y=0) * 0\n",
        "```\n",
        "or given p as success\n",
        "```\n",
        "mean = p * 1 + (1 â€“ p) * 0\n",
        "```\n",
        "Hence,\n",
        "```\n",
        "likelihood = y' * y + (1 â€“ y') * (1 â€“ y)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef7lZyyS8M5u"
      },
      "source": [
        "**Ques. What is the loss function in Logistic Regression using MLE?**\n",
        "\n",
        "**Ans.** We need a loss function that expresses, for an observation x, how close the classifier output ``` (Ë†y = Ïƒ(wÂ· x+b)) ``` is to the correct output (y, which is 0 or 1). \n",
        "\n",
        "Weâ€™ll call this:\n",
        "L(yË†, y) = How much Ë†y differs from the true y\n",
        "\n",
        "We do this via a loss function that prefers the correct class labels of the training examples to be more likely. This is called **conditional maximum likelihood estimation**: we choose the parameters w,b that maximize the log probability of the true y labels in the training data given the observations x. The resulting loss function is the negative log likelihood loss, generally called the **cross-entropy loss**.\n",
        "\n",
        "Derivation :\n",
        "\n",
        "Since there are just two outputs, we can express the conditional probability of y given x as\n",
        "\n",
        "```\n",
        "p(y|x) = y' ^y . (1âˆ’y')^(1âˆ’y). \n",
        "\n",
        "i.e \n",
        "\n",
        "predicted_y to the power of actual y multiplied by 1-predicted y to the poower of (1-actual y)\n",
        "```\n",
        "If y = 0, then p(0|x) = (1-y) \n",
        "\n",
        "If y = 1, then p(1|x) = y\n",
        "\n",
        "Now, taking log both sides - \n",
        "\n",
        "```\n",
        "log likelihood = log (py|x) = y log y' + (1-y) log (1-y') \n",
        "\n",
        "```\n",
        "This function will always return a large probability when the model is close to the matching class value, and a small value when it is far away, for both y=0 and y=1 cases.\n",
        "\n",
        "Finally, we can sum the likelihood function across all examples in the dataset to maximize the likelihood:\n",
        "```\n",
        "maximize sum i to n log(y'_i) * y_i + log(1 â€“ y'_i) * (1 â€“ y_i)\n",
        "```\n",
        "\n",
        "or minimise negative likelihood function or the Cross Entropy Loss, LCE\n",
        "```\n",
        "LCE(yË†, y) = âˆ’[y log Ïƒ(wÂ· x+b) + (1âˆ’y)log(1âˆ’Ïƒ(wÂ· x+b))]\n",
        "\n",
        "\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUpNX5T-vfxs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}