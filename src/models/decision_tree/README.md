**Ques. What is a decision Tree?**

**Ans.**
 Decision Trees are a type of Supervised Machine Learning in which the data is
  continuously split according to a certain parameter. 
  It can be used for both classification(Categorical variable decision tree) 
  and regression ( Continuous Variable Decision Tree).

---

**Ques. What are the assumptions of a Decision Tree?**

**Ans.** Following are the four assumptions of a Decision Tree :

1. In the beginning, the root node has the entire dataset.
2. Decision nodes or the feature values are preferred to be discrete. If they're not, then they are converted to discrete format first and then split is evaluated
3. Data is recursively distributed on the basis of decision nodes
4. Decision nodes are selected on the basis of a statistical metric. This metric governs the quality of split for an attribute.

---

**Ques. What are the parts of a decision tree?**

**Ans.** 
1. Root - Representation of the entire dataset
1. Decision Node - When the dataset is distributed into two on the basis of a feature, that node is called the decision node
1. Leaf / Terminal Node - When there are no more splits possible, those nodes are called the leaf nodes
1. Branch / Sub-Tree: Subpart/subsection of the tree 

---

**Ques. What are the processes related to a decision tree?**

**Ans.**
1. Splitting - Process of distributing the dataset on the basis of a feature based on their value
2. Pruning - When we remove sub-nodes of a decision node, this process is called pruning. This is often done to reduce overfitting. 

---

**Ques. How many types of Decision Trees are there?**

**Ans.**
1. **ID3 (Iterative Dichotomiser 3)** - uses entropy and Information Gain to split
1. **C4.5** - successor of ID3 ; Handles both continuous and discrete attributes; Handles training data with missing attribute values;Prunes trees after creation  
1. **CART (Classification And Regression Tree)** - uses Gini Index to split 
1. **CHAID (CHi-squared Automatic Interaction Detector)** - based on adjusted significance testing
1. **Conditional Inference Trees** Statistics-based approach that uses non-parametric tests as splitting criteria, corrected for multiple testing to avoid overfitting

---

**Ques. What is Entropy?**

**Ans.** Entropy is a measure of the randomness in the information being processed. 
The higher the entropy, the harder it is to draw any conclusions from that information. 

In a fair coin toss, P(head)=1/2 and P(tail)=1/2 hence entropy is at its max. But if there is a two headed coin,
P(head)=1 => no randomness and E(head)=0 i.e., lowest entropy.
```
Entropy for 1 attribute, E(S) = sum over i ( - p_i log(p_i) )

Where 
    S → Current state, and 
    p_i → Probability of an event i of state S or Percentage of class i in a node of state S.
```
Kind of similar to cross entropy!
```
Cross entropy = - p log(p) - (1-p) log (1-p)
where 
    p is probability of dichotomous event
```
If it is extended to p having >2 classes, then we get entropy formula above.
```
Entropy for two attributes(S,A) = sum over class c of outcome in A ( P(c) E(c) )

where 
    S → Current state, and
    A → Chosen attribute
    
```
---

**Ques. What is Information Gain ?**

**Ans.** Information is gained when the entropy is less. 
Hence, Information Gain is the decrease in entropy when an attribute is selected for splitting.
```
Information gain(S,A) = E(S) - E(E,A)

where
    S → Current state, and
    A → Chosen attribute
    and E→ entropy function 
```
IG for all features is calculated and greedily, the feature with max IG is selected at the the decision node.

In other words,

```
IG = Entropy(before) - sum over all k subsets in A ( Entropy(after attribute A) )
 where
    K is the number of subsets generated by the split by taking attribute A
```

---

**Ques. What is Gini Impurity?**

**Ans.** Gini impurity is a measure of how often a randomly chosen element from the set 
would be incorrectly labeled if it was randomly labeled according to the distribution of labels 
in the subset.

**Explanation -** 

Lets say we have a data set with 3 clases of balls -  red(4/10) , blue(3/10) and green(3/10).  
```
The probability that a ball is red is 0.4. 
```
You can only make a mistake about a red ball if the ball is, in fact, red.

Assuming that the guess is based precisely on the probability distribution of the balls,
 then a guess of blue has probability 0.3 and equally a guess of green has probability 0.3. 
 
If the ball really is red, these are the incorrect guesses, since the only other possible guess is correct.

If two events are independent, the probability that both of them occur (P and Q) is the product of their probabilities. If two events are mutually exclusive, then the probability that one of them occurs (P or Q) is the sum of their probabilities.
```
So the probability that a ball is red and is misclassified (as blue or green) is 0.4 * (0.3 + 0.3).
```
To that, we'd have to add the probability of a blue ball being misclassified as 
red or green (0.3 * (0.4 + 0.3)) and the probability of a green ball being 
misclassified as blue or red (0.3 * (0.3 + 0.4)) for a total of 0.66. 

That's extremely close to the maximum value of 2/3 (when all the probabilities are equal). 

Higher the value of Gini impurity, higher is the present heterogeneity. 

Formula
```
Gini impurity = sum( p(i)*(1-p(i))
                = sum(p(i))- sum( p(i)*p(i) )
                = 1- sum (p(i)**2) 

where
    p(i) is the probability of picking the data point with the class i for all classes.
```

CART (Classification and Regression Tree) uses the Gini index method to create split points.

---

**Ques. What are the differences between Information Gain and Gini Impurity?**

**Ans.** Gini Index vs Information Gain

1. The Gini Index facilitates the bigger distributions so easy to implement 
        
    whereas 
    
    the Information Gain favors lesser distributions having small count with multiple specific values.
    
1. The method of the Gini Index is used by CART algorithms, in contrast to it, 
 Information Gain is used in ID3, C4.5 algorithms.

1. Gini index operates on the categorical target variables in terms of “success” or “failure” 
 and performs only binary split, 
in opposite to that Information Gain computes the difference between entropy before and after the split and indicates the impurity in classes of elements. 

---
**Ques. What are the Hyperparameters of a decision tree?**

**Ans.**  